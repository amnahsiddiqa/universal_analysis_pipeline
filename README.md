# Universal data analysis pipeline
[![Build Status](https://travis-ci.com/grst/universal_analysis_pipeline.svg?branch=master)](https://travis-ci.com/grst/universal_analysis_pipeline)

Nextflow-based pipeline to run and deploy reproducible analyses.

## Features
* render jupyter notebooks or Rmarkdown notebooks (papermill/knitr)
* ensure reproducible analyses
* deploy reports to GitHub pages.

## Structure

* `deploy`: final reports. will be uploaded to Github pages
* `nextflow`: nextflow modules
* `lib`: put custom libraries (e.g. python modules)  here
* `notebooks`: The actual analysis steps (i.e. jupyter notebooks, Rmarkdown
  documents) go here.
* `data`: input data.
* `tables`: manually created input data that I want to be under version control.
* `results`: final results generated by the pipeline go here. Concept: one can
  always delete the results directory and re-generate it from `data` using the
  pipeline.
  
## How to run. 
1. Install nextflow 
In this case, we use conda. Check the nextflow webiste for other options. 
```
conda create -n nextflow -c conda-forge -c bioconda nextflow
conda activate nextflow
```

2. Clone this repository
```
gitclone git@github.com:grst/universal_analysis_pipeline.git
cd universal_analysis_pipeline
```

3. Run the pipeline
```
./main.nf
```

4. Deploy the results on github pages. 

The results will be protected by a randomly generated folder name. It is stored in `.access_token`. 
```
./deploy.sh
```







## Ideas
* convert conda envs to singularity containers to ensure reproducibility.
